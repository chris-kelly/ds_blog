---
title: "Bayesian Statistics"
output:
  html_notebook:
    code_folding: hide
    smart: false
  html_document: default
  pdf_document: default
---

```{r, include=FALSE}
# ## GAUSSIAN
# mu <- 50
# sd <- 25
# sample.n <- 1000
# sample.repeat <- lapply(1:10^4, FUN = function(x) {rnorm(n = sample.n, mean = mu, sd = sd)})
# sample.mu <- sapply(sample.repeat, FUN = mean)
# sample.sd <- sapply(sample.repeat, FUN = sd)
# sum(sample.mu - 1.96*sample.sd/sqrt(sample.n) <= mu &
#       sample.mu + 1.96*sample.sd/sqrt(sample.n) >= mu)
# 
# ## BINOMIAL (NORMAL APPROXIMATION)
# p <- 0.5
# sample.n <- 100
# sample.k <- rbinom(n = 10^5, size = sample.n, prob = p)
# sample.p <- sample.k/sample.n
# sum(sample.p-1.96*sqrt(sample.p*(1- sample.p)/sample.n) <= p &
#       sample.p+1.96*sqrt(sample.p*(1- sample.p)/sample.n) >= p)
```

```{r setup, echo = FALSE}
# knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(
	fig.height = 7,
	fig.width = 10,
	message = FALSE,
	warning = FALSE
)
require(plotly)
require(scales)
```

*A Bayesian is one who, vaguely expecting a horse, and catching a glimpse of a donkey, strongly believes he has seen a mule*

## Quick recap of the 'Classical Approach' ('Frequentist')

```{r}
set.seed(0)
p <- 0.5
n <- 10
```

Let's say we have a coin, and we want to know the probability that we flip heads. Frequentists assume that there is only one true, fixed, unknown probability that the coin yields heads. For ease of notation, let's call the constant parameter $p(\text{head}) = \theta$.

To try and discover the true probability of getting a head, we flip the coin `r n` times, and count the number of heads to estimate the probability. Frequentists believe that if we repeated this sampling proceedure many, many times, we should expect the average across all of our samples to be reflective of the true population value (hence why it is called 'frequentist'). 

``` {r, fig.width=10, fig.height=7}
# Create repeated samples
num_repeated_samples <- 10^3
repeated_samples <- rbinom(n = num_repeated_samples, size = n, prob = p)
repeated_samples.cum_p_mean <- cumsum(repeated_samples)/cumsum(rep(n, num_repeated_samples))

# Cumulative mean tends to p
plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = 1:length(repeated_samples), y = repeated_samples.cum_p_mean, name = 'Cumulative mean with sampling') %>%
  layout(showlegend = F
         , yaxis = list(range = c(0, 1))
         , title = "Cumulative sample means tends to true p") # , 
```

``` {r, fig.width=10, fig.height=7}
k <- round(n*p*0.5,0)
sample_mean <- k/n
```

But in the real world, its not possible to take lots of samples over and over again - and we generally just take one. Taking a sample one time, we might observe `r k` heads. This is equivalent to creating a sample of size $`r n`$ and observing a mean probability of $`r sample_mean`$.

We don't know what the true probability of flipping heads is ($\theta$), but we can plot the likelihood of observing `r k` heads for every possible probability of flipping heads the coin could have. On the graph below, for every possible value that the probability of flipping heads could be on the x axis (somewhere between 0 and 1), the likelihood of observing $`r k`$ heads is plotted on the y axis, giving the orange line. 

It appears most probable then that our true probability is indeed `r sample_mean`, which makes sense - the most likely probability is what we observe as the mean in our sample. Thus, a frequentist would conclude that the most likely value of $\theta$ is $`r sample_mean`$.

This is also equivalent to saying that, after observing evidence of the sample mean being $`r sample_mean`$, we estimate that the likelihood of observing $`r k`$ heads is $`r round(dbinom(2, size = 10, prob = sample_mean),4)*100`\%$. This interpretation is important when comparing to the true mean (and later when thinking about Bayesian). In fact, given the true probability distribution of $\theta$ having a mean of $`r p`$, as is plotted in green, the likelihood of observing a sample mean of `r sample_mean` is just $`r round(dbinom(2, size = 10, prob = p),4)*100`\%$

In this example then (and in many others), it can be seen then that just taking one (small) sample can result in an estimated probability that is quite different.

``` {r, fig.width=10, fig.height=7}
# Plotting likelihood of any theta, P(X|theta) (according to frequentists)
binomial_probability <- function(k, n, p) {
  prob_sample <- factorial(n)/(factorial(k)*factorial(n-k))*(p)^k*(1-p)^(n-k)
  return(prob_sample)
}

quantiles <- seq(0,1,length=100)
likelihood_est <- sapply(quantiles, FUN = function(theta) {binomial_probability(p = theta, k = k,  n = n)})
likelihood_act <- sapply(quantiles, FUN = function(theta) {binomial_probability(p = theta, k = n*p,  n = n)})

plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = quantiles, y = likelihood_est, name = 'Estimated from sample') %>%
  add_trace(x = quantiles, y = likelihood_act, name = 'Actual likelihood') %>%
  layout(title = "Likelihood function: X ~ B(n,p)"
         , xaxis = list(title = 'theta')
         , yaxis = list(title = 'likelihood'))
```

In this way then, despite believing the actual (population) probability $\theta$ to be fixed at one value, frequentists believe that the mean of our sample is a random variable (conditioned by $\theta$). By random variable, this means that the number of heads could be a variety of values before sampling (0-`r n`) rather than being pre-determined/fixed. The likelihood of observing any of these possible values is conditioned by the fixed probability $\theta$,  as per the distribution above - (if the true value of $\theta$ is $`r p`$, then observing $`r p*n`$ heads in our sample is a lot more likely than observing $`r n`$).

Hence, to acknowledge that there is randomness associated with this sample mean, frequentists construct confidence intervals. A 95% confidence interval means that, if the sampling proceedure was repeated many, many times, that 95% of the time the true value of $\theta$ would fall within the confidence interval.
*(Technical note - this is not the same as saying that, in one sample, there is a 95% chance that the true value of $\theta$ lies within it. This is because $\theta$ is a fixed value, not a random variable, so it either falls within the sample confidence interval or not.)*

``` {r, fig.width=10, fig.height=7}

true.p.se <- sqrt(p*(1-p)/n)

## BINOMIAL (NORMAL APPROXIMATION)
# The larger the number of successes, np, and failures, n(1-p), the better the normal approximation for confidence intervals
sample.binom.k <- rbinom(n = num_repeated_samples, size = n, prob = p)
sample.binom.p <- sample.binom.k/n
  ## population standard error/confidence interval
    true.binom.p.conf.02.5 <- sample.binom.p-1.96*true.p.se
    true.binom.p.conf.97.5 <- sample.binom.p+1.96*true.p.se
    true.binom.theta_in_conf <- true.binom.p.conf.02.5 < p & true.binom.p.conf.97.5 > p
  ## sample standard error/confidence interval
    sample.binom.p.se <- sqrt(sample.binom.p*(1-sample.binom.p)/n)
    sample.binom.p.conf.02.5 <- sample.binom.p-1.96*sample.binom.p.se
    sample.binom.p.conf.97.5 <- sample.binom.p+1.96*sample.binom.p.se
    sample.binom.theta_in_conf <- sample.binom.p.conf.02.5 < p & sample.binom.p.conf.97.5 > p

## BETA DISTRIBUTION (CONTINUOUS VERSION OF BINOMIAL)
# Since the binomial distribution is non-continuous, we can get confidence intervals closer to 95% from the equivalent beta distribution for the same np & n(1-p)
sample.beta.k <- rbeta(n = num_repeated_samples, shape1 = p*n, shape2 = (1-p)*n)*n
sample.beta.p <- sample.beta.k/n
  ## population standard error/confidence interval
    true.beta.p.conf.02.5 <- sample.beta.p-1.96*true.p.se
    true.beta.p.conf.97.5 <- sample.beta.p+1.96*true.p.se
    true.beta.theta_in_conf <- true.beta.p.conf.02.5 < p & true.beta.p.conf.97.5 > p
  ## sample standard error/confidence interval
    sample.beta.p.se <- sqrt(sample.beta.p*(1-sample.beta.p)/n)
    sample.beta.p.conf.02.5 <- sample.beta.p-1.96*sample.beta.p.se
    sample.beta.p.conf.97.5 <- sample.beta.p+1.96*sample.beta.p.se
    sample.beta.theta_in_conf <- sample.beta.p.conf.02.5 < p & sample.beta.p.conf.97.5 > p

# First one-time sample confidence intervals
one_time.sample.p.se <- sqrt((k/n)*(1-k/n)/n)
one_time_sample.conf_02.5 <- (k/n)-1.96*one_time.sample.p.se
one_time_sample.conf_97.5 <- (k/n)+1.96*one_time.sample.p.se

# One time sample either contains true value of theta or not
plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = rep(c(one_time_sample.conf_02.5, one_time_sample.conf_97.5), each = 3), y = c(1,-1,0,0,-1,1), name = 'Confidence interval') %>%
  add_trace(x = p, y = 0, mode = 'markers', name = 'True value', marker = list(size=50, symbol = 'x')) %>%
  layout(showlegend = F
         , yaxis = list(range = c(-2, 2), zeroline = F, showticklabels = F)
         , xaxis = list(range = c(one_time_sample.conf_02.5 - 0.1, one_time_sample.conf_97.5 + 0.1))
         , title = "One-time sample confidence interval either contains true value of theta or not")

# Cumulative confidence intervals contain 95% of the time
plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = 1:num_repeated_samples
            , y = cumsum(true.beta.theta_in_conf)/cumsum(rep(1,num_repeated_samples))
            , name = 'Cumulative mean with sampling') %>%
  layout(showlegend = F
         #, yaxis = list(range = c(0, 1))
         , title = "Cumulative confidence intervals tends to contain p 95% of the time")

```

In order to obtain these results, certain conditions have to be met, including:

* The randomly sampled observations are independent, so observing any one value does not have an impact on what subsequent values we observe might be. In our example, flipping a head once does not impact the likelihood of flipping it again. *(If running an experiment in practice, this might mean that our population is sufficiently large when compared to our sample that we don't need to be concerned with replacement. Another thing is network effects - imagine an airline increases price of seats as they sell. Say we want to experiment in offering some customers a special offer for airline tickets, our variant, and some not, our control: the customers with the special offer might be more likely to buy, and this will shorten the supply, increasing the price for those without the special offer, and decreasing their likelihood to buy).*
* Any sampled observations are identically distributed to the population *(e.g. if we take our sample from the USA, they might have a different propensity to watch TV than the UK.)*
* Because it is a random sample, the values observed are characterised by a probability distribution associated with $\theta$. *(e.g. we don't know what the number of heads oberved will be before sampling, but the likelihood of what we observe is pre-determined through its associated probability distribution with $\theta$)*

## Okay, so what exactly does a Bayesian approach mean?

Bayesian inference is broadly similar to frequentist inference. **The key difference though is that, under a Bayesian approach, the true value of $\theta$ is treated as a random variable, rather than a fixed constant.** 

So, for example, we might believe that in our economy, $90$% of coins are fair, and $10$% of coins are biased, with biased coins flipping heads $`r k*10`$% of the time. 

Under the guise of being frequentists previously, we had no prior belief as to what $\theta$ would be: we only made an educated guess based on the `r k` heads we observed. We believed that there is one fixed value of the true $\theta$, and that there was only randomness associated with sampling, so we created a confidence interval to portray this.

Now, as bayesians, we are also incorporating our prior beliefs as to what $\theta$ is - that it is either fair or biased. We believe that there is not just random error associated in sampling - we also treat $\theta$ as an additional random variable, since we don't know whether our coin is fair or biased either.

``` {r, fig.width=10, fig.height=7}
fair_coin_k_prob <- binomial_probability(k, n, p)
biased_coin_k_prob <- binomial_probability(k, n, k/n)
```

Given that we observe `r k` heads, the likelihood of this occuring if the coin is fair is $`r percent(fair_coin_k_prob)`$%, and $`r percent(biased_coin_k_prob)`$% if the coin is biased. In the absence of informative prior information then, it might appear that the coin is more likely to be biased.

However, we also need to take into account that $90$% of coins in the population are fair, so even before observing `r k` heads, we have some belief about whether the coin is fair or biased (i.e. the probability distribution associated with the random variable $\theta$). 

``` {r, fig.width=10, fig.height=7}
fair_coin_prob <- (fair_coin_k_prob * 0.9)/((fair_coin_k_prob * 0.9) + (biased_coin_k_prob * 0.1))
biased_coin_prob <- (biased_coin_k_prob * 0.1)/((fair_coin_k_prob * 0.9) + (biased_coin_k_prob * 0.1))
```


$$
P(\text{fair}|H=`r k`) 
=\frac{P(\text{H=`r k` and fair})}{P(\text{H=`r k` and fair})+P(\text{H=`r k` and biased})}
=\frac{P(\text{H=`r k`|fair}) \times P(\text{fair})}{P(\text{H=`r k`})}
$$

$$
P(\text{fair}|H=`r k`)
=\frac{`r round(fair_coin_k_prob,4)` \times 0.9}{`r round(fair_coin_k_prob,4)` + `r round(biased_coin_k_prob,4)`}=`r round(fair_coin_prob, 4)`
$$

$$
P(\text{biased}|H=`r k`) 
=\frac{P(\text{H=`r k` and biased})}{P(\text{H=`r k` and fair})+P(\text{H=`r k` and biased})}
=\frac{P(\text{H=`r k`|biased}) \times  P(\text{biased})}{P(\text{H=`r k`})}
$$
$$
P(\text{biased}|H=`r k`) 
=\frac{`r round(biased_coin_k_prob,4)` \times 0.1}{`r round(fair_coin_k_prob,4)` + `r round(biased_coin_k_prob,4)`}=`r round(biased_coin_prob, 4)`
$$

In other words, our prior belief (before sampling and observing any data) was that the likelihood of our coin having $\theta$ equal to $`r p`$ was $90\%$, and the likelihood of it having theta equal to $`r k/n`$ was $10\%$. After observing `r k` heads, we update our beliefs, so that the likelihood of the probability of our coin flipping heads ($\theta$) being equal to 0.5 is now equal to `r percent(fair_coin_prob)`, and of it being equal to 0.8 being `r percent(biased_coin_prob)`

It is this incorporation of prior data that is either seen as Bayesian's biggest advantage or pitfall. On the one hand, experiments are not abstract devices, and some knowledge about the process being investigated before obtaining the data is known and arguably should be incorporated. On the other, incorporating subjective opinions, particularly strong ones, may mean that you do not learn the true values you are trying to derive. Bayesian is thus analysis that uses a set of observations to change opinion rather than as a means to determine ultimate truth.

To help describe bayesian analysis, the following terms are often used:

* Prior distribution: $Pr(\theta)$ - represents existing belief about $\theta$ (*Represents what was thought before seeing the data*). For example, in the binomial coin example above, we have prior knowledge that 90% of coins in our economy are fair, so $P(\theta)=P(Biased)=0.1$
* Evidence: $X$ - what we just observed ($`r k`$ heads)
* Marginal probability: $Pr(X)$ - the total probability of the data across all possible values of the parameter $\theta$. It doesn't actually depend on $\theta$ and isoften referred to as the proportionality factor/normalising constant (it makes sure all the scenarios are modelled. For example, $P(H=`r k`) = P(H=`r k`|Fair) \times P(Fair) + P(H=`r k`|Biased) \times P(Biased)$)
* Likelihood function: $Pr(X|\theta)$ - the probability of $\theta$ given the data  observed (*Represents the new data available*). For example, $P(H=`r k`|Biased)=`r biased_coin_k_prob`$
* Joint probability density function: $Pr(X,\theta)=Pr((X|\theta).Pr(\theta)$ - used to modify prior beliefs through Bayes Theorem
* Posterior distribution: $Pr(\theta|X)$ - the *posterior density* represents the knowledge about the model parameters after observing the data (*Represents what is now thought given both prior data and data just obtained*)

And thus, we can then describe Bayes Theorem:

$$
P(\theta|X) =\frac{P(X|\theta) \times P(\theta)}{P(X|\theta) \times P(\theta) + P(X|\theta') \times P(\theta')} =\frac{P(X|\theta) \times P(\theta)}{P(X)}
$$

Okay - the example above might not quite be a fair comparison, since the Bayesian example had quite a strong prior: that there were only two possible values for theta, and that 90% of the time it is 0.5.

A more 'objective' prior that is a fairer comparison to the frequentist example is to give all values of theta have an equal likelihood - in other words, a uniform distribution. We can model this using a beta distribution. Going forwards too, let's make this more generic, and term the number of heads observed in the sample as $k$.

*A quick introduction to the beta distribution: it can model lots of different functional forms using $a$ and $b$ as parameters, inputting them into the following function:*

$$
\text{Beta}[a,b] = \theta^{a-1}\times(1-\theta)^{b-1}
$$

So - if we want to create a uniform distribution for our prior, we can model this using a beta distribution by setting $a$ and $b$ to both equal to 1

$$
p(\theta)=\text{Beta}[1,1] = \theta^{1-1}\times(1-\theta)^{1-1}=\theta^{0}\times(1-\theta)^{0}= 1 \text{ (for all values of }\theta)
$$

And we can model our likelihood using the binomial function

$$
p(X| \theta) = p(\text{H=}k| \theta) = \theta^k\times(1-\theta)^{n-k}
$$

(You might have already noticed how eerily similar the beta distribution is to the binomial distribution from the formula) - and in fact we can use this to model it too

$$
p(\text{H=}k| \theta)=\theta^k\times(1-\theta)^{n-k}=\text{Beta}[k,n-k]
$$

And finally - the posterior is proportional to the prior multiplied by the likelihood:

$$
p(\theta) \times p(X| \theta) = [\theta^{a-1}\times(1-\theta)^{b-1}] \times [\theta^k\times(1-\theta)^{n-k}] = \theta^{a-1+k}\times(1-\theta)^{b-1+n-k}
$$

Which we can purely parametize in a beta distribution (you can see that, the stronger the prior - and hence the larger a and b - the smaller the impact new evidence k and n has on the posterior distribution):

$$
\theta^{a-1+k}\times(1-\theta)^{b-1+n-k} = \text{Beta}[a+k,b+n-k] 
$$


``` {r, fig.width=10, fig.height=7}

beta_probability <- function(theta,a,b) {
  return(((theta)^(a-1))*((1-theta)^(b-1)))
}

a <- 1
b <- 1

# uniform distribution
# prior <- dbeta(x = x, shape1 = 1, shape2 = 1)
x <- seq(0, 1, length=100)
prior <- sapply(x, FUN = function(theta) {beta_probability(theta,a,b)})

# binomial distribution
likelihood_f.binom <- sapply(x, FUN = function(theta) {binomial_probability(p = theta, k = k,  n = n)})

# beta distribution
likelihood_f.beta <- sapply(x, FUN = function(theta) {beta_probability(theta,k+1,n-k+1)})

posterior <- sapply(x, FUN = function(theta) {beta_probability(theta,a+k+1,b+n-k+1)})

plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = x, y = prior/sum(prior), name = 'Prior: Uniform = Beta(1,1)') %>%
  add_trace(x = x, y = likelihood_f.binom/sum(likelihood_f.binom), name = 'Likelihood: Binom(k,n-k)') %>%
  add_trace(x = x, y = likelihood_f.beta/sum(likelihood_f.beta), name = 'Likelihood: Beta(p=theta,n)') %>%
  add_trace(x = x, y = posterior/sum(posterior), name = 'Posterior: Beta(a+k,b+n-k)')
```

As you can see, with a very 'objective' prior, the Bayesian posterior is very similar to the result derived from the frequentist evidence-only distribution (the Bayesian likelihood function).

In the two Bayesian examples above though, we either had a very weak prior (the uniform distribution) or a very strong prior (only two available values for theta, and 90% likelihood the coin is fair). Bayesian tends to most valuable where both the new data and past prior have more equal influence, and the posterior forming a distribution that sits somewhere between that estimated by the prior and likelihood function: 

So - let's say the last coin we tried flipping `r n` times got heads `r p*n` times (our prior) and the new coin flipped heads `r k` times (our new evidence). We can then get the following equations:

$$
\text{Prior: } \text{Beta}[k_{1},n_{1}-k_{1}] = \text{Beta}[`r k1`,`r n1`-`r k1`] = \text{Beta}[`r k1`,`r n1-k1`]
$$
$$
\text{Likelihood: } \text{Beta}[k_{2},n_{2}-k_{2}] = \text{Beta}[`r k2`,`r n2`-`r k2`] = \text{Beta}[`r k2`,`r n2-k2`]
$$

$$
\text{Posterior: } \text{Beta}[k_{1}+k_{2},n_{1}-k_{1}+n_{2}-k_{2}] = \text{Beta}[`r k1` + `r k2`,`r n1-k1` + `r n2-k2`] = \text{Beta}[`r k1+k2`,`r n1-k1+n2-k2`]
$$
``` {r, fig.width=10, fig.height=7}
k1 <- 5
n1 <- 10
k2 <- k
n2 <- 10
prior <- sapply(x, FUN = function(theta) {beta_probability(theta,k1,n1-k1)})
likelihood_f <- sapply(x, FUN = function(theta) {beta_probability(theta,k2,n2-k2)})
posterior <- sapply(x, FUN = function(theta) {beta_probability(theta,k1+k2,n1-k1+n2-k2)})

prior <- dbeta(x, k1, n1-k1)
likelihood_f <- dbeta(x, k2, n2-k2)
posterior <- dbeta(x, k1+k2, n1-k1+n2-k2)

plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = x, y = prior, name = 'Prior belief: Horse') %>%
  add_trace(x = x, y = likelihood_f, name = 'Likelihood (New data): Donkey') %>%
  add_trace(x = x, y = posterior, name = 'Posterior Belief: Mule') %>%
  layout(title = "Updating Prior Beliefs")
```

The posterior curve is both taller and narrow than both the distributions estimated from the prior and the likelihood functions. This is because, by including both prior data and new evidence, our sample size has increased, so this reflects greater confidence that the observed number of heads,conditioned by the probability distribution of $\theta$, will lie within a smaller interval.

Which leads on to **Bayesian Credible intervals** - the Bayesian equivalent to frequentist's confidence intervals. Because Bayesian estimates distributions rather than fixed values, credible intervals are arguably more intuitive than confidence intervals: they reflect the a (95%) probability that of here the value generated by the random variable theta will fall within the range (rather than the frequentist interpretation that the true value will fall within the confidence interval 95% of the time).

Like a confidence interval, a credible interval is symmetric around the mean, and spans the x-axis enough to cover 95% of the area under the probability distribution curve. And that is good if the posterior distribution is also symmetric - but as can be seen in our example above, that is not necessarily the case, where the posterior is skewed.

If the distribution is skewed then, it could be better to compute the region of **highest posterior density (HPD)** - which by definition will be the narrowest interval across the potential values for theta, but cover 95% of the area under the probability curve.

``` {r, echo = F, fig.width=10, fig.height=7}
# prior.mean <- sum(x*prior)/sum(prior)
# prior.sd <- sum(prior*(x - prior.mean)^2)/(sum(prior)-1)

# plot_ly(type = 'scatter', mode = 'lines') %>%
#   add_trace(x = x, y = prior, name = 'Prior belief') %>%
#   add_trace(x = x, y = likelihood_f, name = 'Likelihood (New data)') %>%
#   add_trace(x = x, y = posterior, name = 'Posterior Belief') %>%
#   layout(title = "Updating Prior Beliefs")
```



