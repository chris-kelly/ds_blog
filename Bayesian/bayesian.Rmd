---
title: "Bayesian Statistics"
output:
  html_notebook:
    code_folding: hide
    smart: false
  html_document: default
  pdf_document: default
---

```{r setup, echo = FALSE}
# knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(
	fig.height = 7,
	fig.width = 10,
	message = FALSE,
	warning = FALSE
)
require(plotly)
require(scales)
```

*A Bayesian is one who, vaguely expecting a horse, and catching a glimpse of a donkey, strongly believes he has seen a mule*

Why might someone use Bayesian rather than traditional frequentist?

* If they have prior beliefs as to the truth before an experiment is conducted
* If you want to predict how the distribution of a population might change, rather than a point estimate. For example, traditionally, we might try and predict the mean number of people who choose to order after going on our website. 

## Quick recap of the 'Classical Approach' ('Frequentist')

```{r}
set.seed(0)
p <- 0.5
n <- 10
```

Let's say we have a coin, and we want to know the probability that we flip heads. Frequentists assume that there is only one true, fixed, unknown probability that the coin yields heads. For ease of notation, let's call the constant parameter $p(\text{head}) = \theta$.

To try and discover the true probability of getting a head, we flip the coin `r n` times, and count the number of heads to estimate the probability. Frequentists believe that if we repeated this sampling proceedure of 10 flips many, many times, we should expect the average across all of our samples to be reflective of the true population value (hence why it is called 'frequentist'). 

``` {r, fig.width=10, fig.height=7}
# Create repeated samples
num_repeated_samples <- 10^3
repeated_samples <- rbinom(n = num_repeated_samples, size = n, prob = p)
repeated_samples.cum_p_mean <- cumsum(repeated_samples)/cumsum(rep(n, num_repeated_samples))

# Cumulative mean tends to p
plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = 1:length(repeated_samples), y = repeated_samples.cum_p_mean, name = 'Cumulative mean with sampling') %>%
  layout(showlegend = F
         , yaxis = list(range = c(0, 1))
         , title = "Cumulative sample means tends to true p") # , 
```

``` {r, fig.width=10, fig.height=7}
k <- round(n*p*0.5,0) # make up fake k that is 'unlikely'
sample_mean <- k/n

binomial_probability <- function(k, n, p) {
  n_choose_k <- factorial(n)/(factorial(k)*factorial(n-k))
  # NOTE - equivalent to gamma_function(k+n-k)/(gamma_function(k)*gamma_function(n-k), or 1/B(k,n-k)
  prob_sample <- n_choose_k*(p)^k*(1-p)^(n-k)
  return(prob_sample)
}

k_heads_maxprob <- round(binomial_probability(k,n,k/n),4)*100
k_heads_actprob <- round(binomial_probability(k,n,p),4)*100
np_heads_actprob <- round(binomial_probability(n*p,n,p),4)*100

```

But in the real world, its not possible to take lots of samples over and over again - and we generally just take one. Taking a sample one time, we might observe `r k` heads. This is equivalent to creating a sample of size $`r n`$ and observing a mean probability of $`r sample_mean`$.

We don't know what the true probability of flipping heads is ($\theta$), but we can plot the likelihood of observing `r k` heads for every possible probability of flipping heads the coin could have. On the graph below, for every possible value that the probability of flipping heads could be on the x axis (somewhere between 0 and 1), the likelihood of observing $`r k`$ heads is plotted on the y axis, giving the orange line. For example, the likelihood of observing $`r k`$ heads if the probablity of flipping heads is $`r k/n`$ is $`r k_heads_maxprob`\%`$. If the probablity of flipping heads is actually $`r p`$, then the likelihood of flipping `r k` heads in our sample is only $`r k_heads_actprob`\%$. 

It appears most probable from our sample then that our true probability is indeed `r sample_mean`, which makes sense - the most likely probability is what we observe as the mean in our sample. Thus, a frequentist would conclude that the most likely value of $\theta$ is $`r sample_mean`$.

Note that this is also equivalent to saying that, after observing evidence of the sample mean being $`r sample_mean`$, our best estimate for the likelihood of observing different sample means follows the orange probability distribution. This interpretation is important when comparing to the true mean (and later when thinking about Bayesian). In fact, the true probability distribution of sample means conditioned by $\theta$ is plotted in green, having a mean of $`r p`$.

In this example then (and in many others), it can be seen then that just taking one (small) sample can result in an estimated probability that is quite different.

``` {r, fig.width=10, fig.height=7}
x <- seq(0,1,length=100+1)
likelihood_est <- sapply(x, FUN = function(theta) {binomial_probability(p = theta, k = k,  n = n)})
likelihood_act <- sapply(x, FUN = function(theta) {binomial_probability(p = theta, k = n*p,  n = n)})

plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = x, y = likelihood_est, name = 'Estimated from sample') %>%
  add_trace(x = x, y = likelihood_act, name = 'Actual likelihood') %>%
  layout(title = "Likelihood function: X ~ Binom(n,p)"
         , xaxis = list(title = 'theta')
         , yaxis = list(title = 'likelihood'))
```

In this way then, despite believing the actual (population) probability $\theta$ to be fixed at one value, frequentists believe that the mean of our sample is a random variable (conditioned by $\theta$). By random variable, this means that the number of heads could be many different values before sampling (0-`r n`) rather than being pre-determined/fixed. The likelihood of observing any of these possible values is conditioned by the fixed probability $\theta$,  as per the distribution above - (if the true value of $\theta$ is $`r p`$, then observing $`r p*n`$ heads in our sample is a lot more likely than observing $`r n`$).

Hence, to acknowledge that there is randomness associated with this sample mean, frequentists construct confidence intervals. Again to follow the frequentist interpretation, a 95% confidence interval means that, if the sampling proceedure was repeated many, many times, that 95% of the time the true value of $\theta$ would fall within the confidence interval.
*(Technical note - this is not the same as saying that, in one sample, there is a 95% chance that the true value of $\theta$ lies within it. This is because $\theta$ is a fixed value, not a random variable, so it either falls within the sample confidence interval or not.)*

``` {r, fig.width=10, fig.height=7}

true.p.se <- sqrt(p*(1-p)/n)

## BINOMIAL (NORMAL APPROXIMATION)
# The larger the number of successes, np, and failures, n(1-p), the better the normal approximation for confidence intervals (p near 0.5, n as large as possible)
sample.binom.k <- rbinom(n = num_repeated_samples, size = n, prob = p)
sample.binom.p <- sample.binom.k/n
  ## population standard error/confidence interval
    true.binom.p.conf.02.5 <- sample.binom.p-1.96*true.p.se
    true.binom.p.conf.97.5 <- sample.binom.p+1.96*true.p.se
    true.binom.theta_in_conf <- true.binom.p.conf.02.5 < p & true.binom.p.conf.97.5 > p
  ## sample standard error/confidence interval
    sample.binom.p.se <- sqrt(sample.binom.p*(1-sample.binom.p)/n)
    sample.binom.p.conf.02.5 <- sample.binom.p-1.96*sample.binom.p.se
    sample.binom.p.conf.97.5 <- sample.binom.p+1.96*sample.binom.p.se
    sample.binom.theta_in_conf <- sample.binom.p.conf.02.5 < p & sample.binom.p.conf.97.5 > p

## BETA DISTRIBUTION (CONTINUOUS VERSION OF BINOMIAL)
# Since the binomial distribution is non-continuous, we can get confidence intervals closer to 95% from the equivalent beta distribution for the same np & n(1-p)
sample.beta.k <- rbeta(n = num_repeated_samples, shape1 = p*n, shape2 = (1-p)*n)*n
sample.beta.p <- sample.beta.k/n
  ## population standard error/confidence interval
    true.beta.p.conf.02.5 <- sample.beta.p-1.96*true.p.se
    true.beta.p.conf.97.5 <- sample.beta.p+1.96*true.p.se
    true.beta.theta_in_conf <- true.beta.p.conf.02.5 < p & true.beta.p.conf.97.5 > p
  ## sample standard error/confidence interval
    sample.beta.p.se <- sqrt(sample.beta.p*(1-sample.beta.p)/n)
    sample.beta.p.conf.02.5 <- sample.beta.p-1.96*sample.beta.p.se
    sample.beta.p.conf.97.5 <- sample.beta.p+1.96*sample.beta.p.se
    sample.beta.theta_in_conf <- sample.beta.p.conf.02.5 < p & sample.beta.p.conf.97.5 > p

# First one-time sample confidence intervals
one_time.sample.p.se <- sqrt((k/n)*(1-k/n)/n)
one_time_sample.conf_02.5 <- (k/n)-1.96*one_time.sample.p.se
one_time_sample.conf_97.5 <- (k/n)+1.96*one_time.sample.p.se

# One time sample either contains true value of theta or not
plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = rep(c(one_time_sample.conf_02.5, one_time_sample.conf_97.5), each = 3), y = c(1,-1,0,0,-1,1), name = 'Confidence interval') %>%
  add_trace(x = p, y = 0, mode = 'markers', name = 'True value', marker = list(size=50, symbol = 'x')) %>%
  layout(showlegend = F
         , yaxis = list(range = c(-2, 2), zeroline = F, showticklabels = F)
         , xaxis = list(range = c(one_time_sample.conf_02.5 - 0.1, one_time_sample.conf_97.5 + 0.1))
         , title = "One-time sample confidence interval either contains true value of theta or not")

# Cumulative confidence intervals contain 95% of the time
plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = 1:num_repeated_samples
            , y = cumsum(true.beta.theta_in_conf)/cumsum(rep(1,num_repeated_samples))
            , name = 'Cumulative mean with sampling') %>%
  layout(showlegend = F
         #, yaxis = list(range = c(0, 1))
         , title = "Cumulative confidence intervals tends to contain p 95% of the time")

```

In order to obtain these results, certain conditions have to be met, including:

* The randomly sampled observations are independent, so observing any one value does not have an impact on what subsequent values we observe might be. In our example, flipping a head once does not impact the likelihood of flipping it again. *(If running an experiment in practice, this might mean that our population is sufficiently large when compared to our sample that we don't need to be concerned with replacement. Another thing to watch out for is network effects - imagine an airline increases price of seats as they sell. Say we want to experiment in offering some customers a special offer for airline tickets, our variant, and some not, our control: the customers with the special offer might be more likely to buy, and this will shorten the supply, increasing the price for those without the special offer, and decreasing their likelihood to buy).*
* Any sampled observations are identically distributed to the population *(e.g. if we take our sample from the USA, they might have a different propensity to watch TV than the UK.)*
* Because it is a random sample, the values observed are characterised by a probability distribution associated with $\theta$. *(e.g. we don't know what the number of heads oberved will be before sampling, but the likelihood of what we observe is pre-determined through its associated probability distribution with $\theta$)*

## Okay, so what exactly does a Bayesian approach mean?

Bayesian inference is broadly similar to frequentist inference. **The key difference though is that, under a Bayesian approach, the true value of $\theta$ is treated as a random variable, rather than a fixed constant.** 

So, for example, we might believe that in our economy, $90$% of coins are fair, and $10$% of coins are biased, with biased coins flipping heads $`r k*10`$% of the time. 

Under the guise of being frequentists previously, we had no prior belief as to what $\theta$ would be: we only made an educated guess based on the `r k` heads we observed. We believed that there is one fixed value of the true $\theta$, and that there was only randomness associated with sampling, so we created a confidence interval to portray this.

Now, as bayesians, we are also incorporating our prior beliefs as to what $\theta$ is - that it is either fair or biased. We believe that there is not just random error associated in sampling - we also treat $\theta$ as an additional random variable, since we don't know whether our coin is fair or biased either, and fair coins will have a different sample mean distribution than biased ones.

``` {r, fig.width=10, fig.height=7}
fair_coin_k_prob <- binomial_probability(k, n, p)
biased_coin_k_prob <- binomial_probability(k, n, k/n)
```

Given that we observe `r k` heads, the likelihood of this occuring if the coin is fair is $`r percent(fair_coin_k_prob)`$%, and $`r percent(biased_coin_k_prob)`$% if the coin is biased. In the absence of informative prior information then, it might appear that the coin is more likely to be biased.

However, we also need to take into account that $90$% of coins in the population are fair, so even before observing `r k` heads, we have some belief about whether the coin is fair or biased (i.e. the probability distribution associated with the random variable $\theta$). 

``` {r, fig.width=10, fig.height=7}
fair_coin_prob <- (fair_coin_k_prob * 0.9)/((fair_coin_k_prob * 0.9) + (biased_coin_k_prob * 0.1))
biased_coin_prob <- (biased_coin_k_prob * 0.1)/((fair_coin_k_prob * 0.9) + (biased_coin_k_prob * 0.1))

# data.frame(
# matrix(data = c(binomial_probability(2,10,0.5)*0.9 # p(fair, 2 heads)
#                 , (1-binomial_probability(2,10,0.5))*0.9 # p(fair, not 2 heads)
#                 , 0.9 # p(fair)
#                 , binomial_probability(2,10,0.2)*0.1 # biased, 2 heads
#                 , (1-binomial_probability(2,10,0.2))*0.1 # biased, not 2 heads
#                 , 0.1
#                 , binomial_probability(2,10,0.5)*0.9 + binomial_probability(2,10,0.2)*0.1
#                 , (1-binomial_probability(2,10,0.5))*0.9 + (1-binomial_probability(2,10,0.2))*0.1
#                 , 1
#                 )
#        , nrow = 3
#        , dimnames = list(c('Flip 2 heads', 'Not flip 2 heads', 'Total')
#                          , c('Fair', 'Biased', 'Total'))
#        )
# )
```

..and so we can find the likelihood we got a fair coin, given the fact we observed 2 heads:

$$
P(\text{fair}|H=`r k`) 
=\frac{P(\text{H=`r k` and fair})}{P(\text{H=`r k` and fair})+P(\text{H=`r k` and biased})}
=\frac{P(\text{H=`r k`|fair}) \times P(\text{fair})}{P(\text{H=`r k`})}
$$

$$
P(\text{fair}|H=`r k`)
=\frac{`r round(fair_coin_k_prob,4)` \times 0.9}{`r round(fair_coin_k_prob,4)` + `r round(biased_coin_k_prob,4)`}=`r round(fair_coin_prob, 4)`
$$

$$
P(\text{biased}|H=`r k`) 
=\frac{P(\text{H=`r k` and biased})}{P(\text{H=`r k` and fair})+P(\text{H=`r k` and biased})}
=\frac{P(\text{H=`r k`|biased}) \times  P(\text{biased})}{P(\text{H=`r k`})}
$$
$$
P(\text{biased}|H=`r k`) 
=\frac{`r round(biased_coin_k_prob,4)` \times 0.1}{`r round(fair_coin_k_prob,4)` + `r round(biased_coin_k_prob,4)`}=`r round(biased_coin_prob, 4)`
$$

In other words, our prior belief (before sampling and observing any data) was that the likelihood of our coin having $\theta$ equal to $`r p`$ was $90\%$, and the likelihood of it having theta equal to $`r k/n`$ was $10\%$. After observing `r k` heads, we update our beliefs, so that the likelihood of the probability of our coin flipping heads ($\theta$) being equal to 0.5 is now equal to `r percent(fair_coin_prob)`, and of it being equal to `r sample_mean` being `r percent(biased_coin_prob)`

It is this incorporation of prior data that is either seen as Bayesian's biggest advantage or pitfall. On the one hand, experiments are not abstract devices, and some knowledge about the process being investigated before obtaining the data is known and arguably should be incorporated. On the other, incorporating subjective opinions, particularly strong ones, may mean that you do not learn the true values you are trying to derive. Bayesian is thus analysis that uses a set of observations to change opinion rather than as a means to determine ultimate truth.

To help describe bayesian analysis, the following terms are often used:

* Prior distribution: $Pr(\theta)$ - represents existing belief about $\theta$ (*Represents what was thought before seeing the data*). For example, in the binomial coin example above, we have prior knowledge that 90% of coins in our economy are fair, so $P(\theta)=P(Biased)=0.1$
* Evidence: $X$ - what we just observed ($`r k`$ heads)
* Marginal probability: $Pr(X)$ - the total probability of the data across all possible values of the parameter $\theta$. It doesn't actually depend on $\theta$ and isoften referred to as the proportionality factor/normalising constant (it makes sure all the scenarios are modelled. For example, $P(H=`r k`) = P(H=`r k`|Fair) \times P(Fair) + P(H=`r k`|Biased) \times P(Biased)$)
* Likelihood function: $Pr(X|\theta)$ - the probability of $\theta$ given the data  observed (*Represents the new data available*). For example, $P(H=`r k`|Biased)=`r biased_coin_k_prob`$
* Joint probability density function: $Pr(X,\theta)=Pr((X|\theta).Pr(\theta)$ - used to modify prior beliefs through Bayes Theorem
* Posterior distribution: $Pr(\theta|X)$ - the *posterior density* represents the knowledge about the model parameters after observing the data (*Represents what is now thought given both prior data and data just obtained*)

And thus, we can then describe Bayes Theorem:

$$
P(\theta|X) =\frac{P(X|\theta) \times P(\theta)}{P(X|\theta) \times P(\theta) + P(X|\theta') \times P(\theta')} =\frac{P(X|\theta) \times P(\theta)}{P(X)}
$$

Okay - the example above might not quite be a fair comparison, since the Bayesian example had quite a strong prior: that there were only two possible values for theta, and that 90% of the time it is 0.5.

A more 'objective' prior that is a fairer comparison to the frequentist example is to give all values of theta have an equal likelihood - in other words, a uniform distribution. We can model this using a beta distribution. Going forwards too, let's make this more generic, and term the number of heads observed in the sample as $k$.

*A quick introduction to the beta distribution: it can model lots of different functional forms using $a$ and $b$ as parameters, by inputting them into the following function:*

$$
\text{Beta}[a,b] \sim \theta^{a-1}\times(1-\theta)^{b-1}
$$

So - if we want to create a uniform distribution for our prior, we can model this using a beta distribution by setting $a$ and $b$ to both equal to 1

$$
p(\theta) \sim \text{Beta}[1,1] \sim \theta^{1-1}\times(1-\theta)^{1-1}=\theta^{0}\times(1-\theta)^{0}= 1 \text{ (for all values of }\theta)
$$

And we can model our likelihood using the binomial function

$$
p(X| \theta) = p(\text{H=}k| \theta) = \text{Binom}(n,p) \sim \theta^k\times(1-\theta)^{n-k}
$$

And finally - the posterior is proportional to the prior multiplied by the likelihood:

$$
p(\theta) \times p(X| \theta) \sim [\theta^{a-1}\times(1-\theta)^{b-1}] \times [\theta^k\times(1-\theta)^{n-k}] = \theta^{a-1+k}\times(1-\theta)^{b-1+n-k}
$$

Which we can purely parametize in a beta distribution (you can see that, the stronger the prior - and hence the larger a and b - the smaller the impact new evidence k and n has on the posterior distribution):

$$
\theta^{a-1+k}\times(1-\theta)^{b-1+n-k} \sim \text{Beta}[a+k,b+n-k] 
$$

(Note this is an approximation - the actual equality is specified later)

``` {r, fig.width=10, fig.height=7}

gamma_function <- function(n) {
  return(factorial(n-1))
}

beta_probability <- function(theta,a,b) {
  numerator <- ((theta)^(a-1))*((1-theta)^(b-1))
  denominator <- (gamma_function(a)*gamma_function(b))/gamma_function(a+b)
  return(numerator/denominator)
}

a <- 1
b <- 1

# uniform distribution
# prior <- dbeta(x = x, shape1 = 1, shape2 = 1)
x <- seq(0, 1, length=100+1)
prior <- sapply(x, FUN = function(theta) {beta_probability(theta,a,b)})

# binomial distribution
likelihood_f.binom <- sapply(x, FUN = function(theta) {binomial_probability(k,n,theta)})
likelihood_f.binom.int <- likelihood_f.binom
likelihood_f.binom.int[round(x*n,0) != x*n] <- NA

# beta distribution
likelihood_f.beta <- sapply(x, FUN = function(theta) {beta_probability(theta,k+1,n-k+1)*1/(n+1)})

posterior <- sapply(x, FUN = function(theta) {beta_probability(theta,a+k+1,b+n-k+1)*1/(n+1)})

plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = x, y = prior/sum(prior), name = 'Prior: Uniform = Beta(1,1)') %>%
  add_trace(x = x, y = likelihood_f.binom.int, name = 'Likelihood: ~ Binom(k=2,p=theta,n)', mode = 'markers') %>%
  add_trace(x = x, y = likelihood_f.beta, name = 'Likelihood: ~ Beta(p=theta,k,n-k)') %>%
  add_trace(x = x, y = posterior, name = 'Posterior: ~ Beta(a+k,b+n-k)') %>%
  layout(xaxis = list(title = 'Theta'))
```

As you can see, with a very 'objective' prior, the Bayesian posterior is very similar to the result derived from the frequentist evidence-only distribution (the Bayesian likelihood function).

In the two Bayesian examples above though, we either had a very weak prior (the uniform distribution) or a very strong prior (only two available values for theta, and 90% likelihood the coin is fair). Bayesian tends to most valuable where both the new data and past prior have more equal influence, and the posterior forming a distribution that sits somewhere between that estimated by the prior and likelihood function: 

So - let's say the last coin we tried flipping `r n` times got heads `r p*n` times (our prior) and the new coin flipped heads `r k` times (our new evidence). We can then get the following equations:

$$
\text{Prior: } \text{Beta}[k_{1},n_{1}-k_{1}] = \text{Beta}[`r k1`,`r n1`-`r k1`] = \text{Beta}[`r k1`,`r n1-k1`]
$$
$$
\text{Likelihood: } \text{Beta}[k_{2},n_{2}-k_{2}] = \text{Beta}[`r k2`,`r n2`-`r k2`] = \text{Beta}[`r k2`,`r n2-k2`]
$$

$$
\text{Posterior: } \text{Beta}[k_{1}+k_{2},n_{1}-k_{1}+n_{2}-k_{2}] = \text{Beta}[`r k1` + `r k2`,`r n1-k1` + `r n2-k2`] = \text{Beta}[`r k1+k2`,`r n1-k1+n2-k2`]
$$
``` {r, fig.width=10, fig.height=7}
k1 <- 5
n1 <- 10
k2 <- k
n2 <- 10

# prior <- sapply(x, FUN = function(theta) {beta_probability(theta,k1+1,n1-k1+1)/(n1+1)})
# likelihood_f <- sapply(x, FUN = function(theta) {beta_probability(theta,k2+1,n2-k2+1)/(n2+1)})
# posterior <- sapply(x, FUN = function(theta) {beta_probability(theta,k1+k2+2,n1-k1+1+n2-k2+1)/(n1+n2+2)})

# prior <- dbeta(x, k1+1, n1-k1+1)/(n1+1)
# likelihood_f <- dbeta(x, k2+1, n2-k2+1)/(n2+1)
# posterior <- dbeta(x, k1+k2+1, n1-k1+n2-k2+1)/(n1+n2+1)

prior <- dbeta(x, k1, n1-k1)
likelihood_f <- dbeta(x, k2, n2-k2)
posterior <- dbeta(x, k1+k2, n1-k1+n2-k2)

plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = x, y = prior, name = 'Prior belief: Horse') %>%
  add_trace(x = x, y = likelihood_f, name = 'Likelihood (New data): Donkey') %>%
  add_trace(x = x, y = posterior, name = 'Posterior Belief: Mule') %>%
  layout(title = "Updating Prior Beliefs")
```

The posterior curve is both taller and narrow than both the distributions estimated from the prior and the likelihood functions. This is because, by including both prior data and new evidence, our sample size has increased, so this reflects greater confidence that the observed number of heads,conditioned by the probability distribution of $\theta$, will lie within a smaller interval.

Which leads on to **Bayesian Credible intervals** - the Bayesian equivalent to frequentist's confidence intervals. Because Bayesian estimates distributions rather than fixed values, credible intervals are arguably more intuitive than confidence intervals: they reflect the a (95%) probability that of here the value generated by the random variable theta will fall within the range (rather than the frequentist interpretation that the true value will fall within the confidence interval 95% of the time).

Like a confidence interval, a credible interval is symmetric around the mean, and spans the x-axis enough to cover 95% of the area under the probability distribution curve. And that is good if the posterior distribution is also symmetric - but as can be seen in our example above, that is not necessarily the case, where the posterior is skewed.

If the distribution is skewed then, it could be better to compute the region of **highest posterior density (HPD)** - which by definition will be the narrowest interval across the potential values for theta, but cover 95% of the area under the probability curve. In other words, we still maintain the same 95% probability of theta lying within the region, but the region is the narrowest it possibly can be.

So to do this, we want to find the interval between $\theta_{low}$ and $\theta_{high}$ such at:

$$
\int_{\theta_{low}}^{\theta_{high}} p(\theta|x) d\theta= 1-\alpha
$$

But finding this can be tricky, particularly with computing closed intervals. The code below shows a brute force solution. The way to think about this is as follows:
* Credible intervals: Given the mean of the distribution, expand the range outwards equally until 95% of the area under the curve is captured.
* Highest posterior density: Imagine a horizontal line moving slowly towards the x axis, intercepting the distribution twice. Once the 

```{r}
find_binomial_credible_interval <- function(k, n, p, learning_rate = 0.5, initial_width = 0.1, criterion = 0.1, max_iterations = 1000, precision = 4) {
  bit <- initial_width/2 # amount to push away from mean
  for(i in 1:max_iterations) {
    amount <- pbeta(p+bit, k+1, n-k+1) - pbeta(p-bit, k+1, n-k+1) # sum the auc between p-bit and p+bit
    bit <- bit - learning_rate*(amount - (1-criterion)) # decrease if auc > 0.95 or increase if auc < 0.85 
    if(abs(amount - (1-criterion)) < 1/(10^precision)) {
      break()
    }
  }
  return(c(p-bit,p+bit))
}

find_binomial_highest_start_point <- function(precision = 4,k,n) {
  x <- seq(0,1,10^-precision)
  pdf <- dbeta(x,k,n-k)
  return(x[max(pdf) == pdf])
}

find_binomial_hpd <- function(k,n,criterion=0.1, initial_movement_size = 0.1, max_iterations=10^3,learning_rate =  0.005, precision = 4) {
  p <- find_binomial_highest_start_point(precision,k,n)
  left_loc <- p
  right_loc <- p
  residual <- (1-criterion) - (pbeta(right_loc, k+1, n-k+1)-pbeta(left_loc, k+1, n-k+1))
  bit <- initial_movement_size
  for(i in 1:1000) {
    amount_right <- pbeta(right_loc+bit,k+1,n-k+1) - pbeta(right_loc, k+1, n-k+1)
    amount_left <- pbeta(left_loc, k+1, n-k+1) - pbeta(left_loc-bit, k+1, n-k+1)
    if(residual > 0) { # need to increase width
      if(amount_left > amount_right) { # moving more left is more efficient
        left_loc <- left_loc-bit
      } else {
        right_loc <- right_loc+bit;
      }
    } else { # need to decrease width
      if(amount_left > amount_right) { # moving less right is least efficient
        right_loc <- right_loc-bit;
      } else {
        left_loc <- left_loc+bit
      }
    }
    residual <- (1-criterion) - (pbeta(right_loc, k+1, n-k+1)-pbeta(left_loc, k+1, n-k+1))
    if(abs(residual) < 10^-precision) {
      break()
    } else {
      bit <- bit - learning_rate*residual
    }
  }
  return(c(left_loc, right_loc))
}

credible_intervals <- find_binomial_credible_interval(k, n, p, max_iterations = 10^3, criterion = 0.1)
highest_posterior_density <- find_binomial_hpd(k,n, max_iterations = 10^3, criterion = 0.1)

plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = x, y = likelihood_f, name = 'Binom(10,0.2)') %>%
  add_trace(x = rep(p,2), y = c(0,max(likelihood_f)), name = 'Mean') %>%
  # add_trace(x = rep(credible_intervals[1],2), y = c(0,max(likelihood_f)), name = 'Lower credible interval bound') %>%
  # add_trace(x = rep(credible_intervals[2],2), y = c(0,max(likelihood_f)), name = 'Upper credible interval bound') %>%
  add_trace(x = pmin(pmax(x, credible_intervals[1]),credible_intervals[2]), y = likelihood_f, name = 'Credible Interval', fill = 'tozeroy', line = list(dash = 'dot')) %>%
  layout(title = 'Credible Interval (equidistant around mean)')

plot_ly(type = 'scatter', mode = 'lines') %>%
  add_trace(x = x, y = likelihood_f, name = 'Binom(10,0.2)') %>%
  add_trace(x = rep(p,2), y = c(0,max(likelihood_f)), name = 'Mean') %>%
  # add_trace(x = rep(credible_intervals[1],2), y = c(0,max(likelihood_f)), name = 'Lower credible interval bound') %>%
  # add_trace(x = rep(credible_intervals[2],2), y = c(0,max(likelihood_f)), name = 'Upper credible interval bound') %>%
  add_trace(x = pmin(pmax(x, highest_posterior_density[1]),highest_posterior_density[2]), y = likelihood_f, name = 'Highest Posterior Density', fill = 'tozeroy', line = list(dash = 'dot')) %>%
  layout(title = 'Highest Posterior Density (smallest spread that contains 95% of data)')
  
```

Some Bayesians argue though that credible intervals and HPDs are only really something that are useful to compare to frequentist interpretations, as the distribution is already given by the posterior.

## Thinking about Bayesian in terms of predicting things

There are two sources of uncertaint when building statistical models to predict things:

* Uncertainty in the parameter values which have been estimated on the basis of past data (for example, a coefficient estimated in a regression model)
* Uncertainty due to the fact any future value is itself a random event (because future events are essentially samples that have error conditioned by $\theta$)

Classical models deal with point 2, making a point estimate in the future based on the parameters estimated from past data. This is equivalent to forming a likelihood (i.e. incorporating new data).
What they don't do though is think about point 1, that the model estimated itself might be incorrect since the true values of $\theta$ are not fixed, and this is equivalent to not building in any prior beliefs. In a way, by giving a point estimate, it generates a false sense of precision.



``` {r, echo = F, fig.width=10, fig.height=7}



```



Now for the geeks - you might have already noticed how eerily similar the beta distribution is to the binomial distribution from the formula - and in fact you can use the beta distribution as a continous function equivalent to the binomial function:

$$
p(\text{H=}k| \theta) \sim \theta^k\times(1-\theta)^{n-k} \sim \text{Beta}[k+1,n-k+1]
$$

Both binomial and beta likelihoods are scaled though (normalized), so that the sum of their probabilites total 1 though.
For the beta distribution, the scaling is achieved like this:

$$
Beta[\alpha,\beta] = \theta^{\alpha-1}\times(1-\theta)^{\beta-1} \times \frac{1}{\text{B}(\alpha,\beta)}
$$
Where

$$
\frac{1}{\text{B}(\alpha,\beta)} = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \times \Gamma(\beta)} = \frac{(\alpha+\beta-1)!}{(\alpha-1)! \times (\beta-1)!}
$$
So if successes $ \alpha = k+1 $ and failures $ \beta = n - k + 1$, then:

$$
\frac{(\alpha+\beta-1)!}{(\alpha-1)! \times (\beta-1)!}=\frac{((k+1)+(n-k+1)-1)!}{((k+1)-1)! \times ((n-k+1)-1)!}=\frac{(n+1)!}{(k)!\times(n-k)!} =
{n+1 \choose k } = (n+1) \times {n \choose k }
$$
And thus:

$$
Beta[k+1,n-k+1] = (n+1) \times \text{Binom}(n,p)
$$

